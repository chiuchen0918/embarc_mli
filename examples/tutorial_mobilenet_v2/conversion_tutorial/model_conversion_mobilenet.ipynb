{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Frozen Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import array\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, ds_info = tfds.load('imagenet_v2', split='test', as_supervised=True, with_info=True)\n",
    "ds = tfds.as_numpy(ds)\n",
    "\n",
    "test_labels = []\n",
    "img_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SAMPLES = 100\n",
    "IMAGE_SIDE = 96\n",
    "\n",
    "def dumpLayers(model):\n",
    "    for layer in model.layers:\n",
    "        print(layer.name)\n",
    "        break\n",
    "\n",
    "def getModel():\n",
    "    model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "        input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3), alpha=0.35,\n",
    "        include_top=True, classifier_activation='softmax', weights='imagenet', pooling='avg'\n",
    "    )\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocess(image):\n",
    "    image = (image) / 127.5\n",
    "    return image - 1.0\n",
    "\n",
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    _,ret = cv2.imencode('.jpg', img) \n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)\n",
    "\n",
    "def quantizationDataGenerator():\n",
    "    count = 0\n",
    "    for image, label in ds:\n",
    "        image = image.copy()\n",
    "        image = cv2.resize(image, (IMAGE_SIDE, IMAGE_SIDE))\n",
    "        array = preprocess(image).astype(np.float32)[np.newaxis, ...]\n",
    "        yield([array])\n",
    "        count += 1\n",
    "        if count > 300:\n",
    "            break\n",
    "\n",
    "def testDataGenerator():\n",
    "    count = 0\n",
    "    for image, label in ds:\n",
    "        image = image.copy()\n",
    "        image = cv2.resize(image, (IMAGE_SIDE, IMAGE_SIDE))\n",
    "        imshow(image)\n",
    "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite('img/' + str(label) + '.jpg', img_rgb, [cv2.IMWRITE_JPEG_QUALITY,100])\n",
    "        print(img_rgb)\n",
    "        array = preprocess(image).astype(np.float32)[np.newaxis, ...]\n",
    "        print(label)\n",
    "        test_labels.append(label)\n",
    "        yield(array)\n",
    "        count += 1\n",
    "        if count > 300:\n",
    "            break\n",
    "        \n",
    "def convertTFL(name, model):\n",
    "    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('generated/mobilenet/mobilenet_v2_0.35_96_frozen.pb', input_arrays = ['input'], output_arrays = ['MobilenetV2/Predictions/Reshape'], input_shapes = {'input' : [1, 96, 96, 3]})\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = quantizationDataGenerator\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    quantModel = converter.convert()\n",
    "\n",
    "    with open(\"{}.tflite\".format(name), \"wb\") as f:\n",
    "        f.write(quantModel)\n",
    "        \n",
    "def runTFL(name):\n",
    "    from tflite_runtime.interpreter import Interpreter\n",
    "    interpreter = Interpreter('{}.tflite'.format(name))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    inputTensorIndex = interpreter.get_input_details()[0]['index']\n",
    "    outputTensorIndex = interpreter.get_output_details()[0]['index']\n",
    "    input_scale, input_zero_point = interpreter.get_input_details()[0]['quantization']\n",
    "    output_scale, output_zero_point = interpreter.get_output_details()[0]['quantization']\n",
    "    \n",
    "    print(input_scale)\n",
    "    print(input_zero_point)\n",
    "    \n",
    "    prediction_values = []\n",
    "    \n",
    "    for x in testDataGenerator():\n",
    "        test_image = np.int8(x / input_scale + input_zero_point)\n",
    "        interpreter.set_tensor(inputTensorIndex, test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(outputTensorIndex)\n",
    "        result = np.argmax(output[0])\n",
    "        prediction_values.append(result - 1)\n",
    "    \n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_values)):\n",
    "        print(str(prediction_values[index]) + \":\" + str(test_labels[index]))\n",
    "        if prediction_values[index] == test_labels[index]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_values)\n",
    "\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "name = 'mobilenet_v2'\n",
    "model = getModel()\n",
    "model.summary()\n",
    "convertTFL(name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runTFL(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = tf.keras.utils.get_file(\n",
    "    \"mountains.jpg\",\n",
    "    \"https://storage.googleapis.com/gcptutorials.com/examples/mountains.jpg\")\n",
    "img = tf.keras.preprocessing.image.load_img(file, target_size=[96, 96])\n",
    "\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "print(x.shape)\n",
    "imshow(x)\n",
    "x = tf.keras.applications.mobilenet.preprocess_input(x[tf.newaxis,...])\n",
    "print(x.shape)\n",
    "\n",
    "labels_path = tf.keras.utils.get_file(\n",
    "    'ImageNetLabels.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "labels = np.array(open(labels_path).read().splitlines())\n",
    "print(len(labels))\n",
    "\n",
    "predictions = model(x)\n",
    "print(predictions.shape)\n",
    "\n",
    "top_5_classes_index = np.argsort(predictions)[0 , ::-1][:5]+1\n",
    "\n",
    "print(top_5_classes_index)\n",
    "\n",
    "top_5_classes = labels[top_5_classes_index]\n",
    "print(top_5_classes)\n",
    "\n",
    "\n",
    "\n",
    "from tflite_runtime.interpreter import Interpreter\n",
    "interpreter = Interpreter('mobilenet_v2.tflite'.format(name))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "inputTensorIndex = interpreter.get_input_details()[0]['index']\n",
    "outputTensorIndex = interpreter.get_output_details()[0]['index']\n",
    "scale, zero_point = interpreter.get_input_details()[0]['quantization']\n",
    "\n",
    "print(scale)\n",
    "print(zero_point)\n",
    "\n",
    "prediction_values = []\n",
    "\n",
    "test_image = np.int8(x / scale + zero_point)\n",
    "interpreter.set_tensor(inputTensorIndex, test_image)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(outputTensorIndex)\n",
    "result = np.argmax(output[0])\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
